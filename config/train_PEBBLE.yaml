defaults:
    - agent: sac

# obs process
process_type: 2 # 0 for not process, 1 for single state, 2 for state with last state
traj_action: False
traj_save_path: buffer 

# this needs to be specified manually
experiment: PEBBLE

# reward learning
vlm_label: gpt-4o-mini-2024-07-18   # gpt-4o,gpt-4o-mini-2024-07-18
better_traj_gen: True
double_check: False
save_equal: False
vlm_feedback: True
generate_check: False


segment: 10
activation: tanh
num_seed_steps: 1000
num_unsup_steps: 10 #1000
num_interact: 1000 #5000
reward_lr: 0.0003
reward_batch: 3 #128
reward_update: 200
feed_type: 0 # 0
reset_update: 100
topK: 5
ensemble_size: 3
max_feedback: 1400
large_batch: 10
label_margin: 0.0
teacher_beta: -1
teacher_gamma: 1
teacher_eps_mistake: 0
teacher_eps_skip: 0
teacher_eps_equal: 0

# scheduling
reward_schedule: 0

num_train_steps: 1e6
replay_buffer_capacity: ${num_train_steps}

# evaluation config
eval_frequency: 10000 #10000
num_eval_episodes: 10
device: cuda

# logger
log_frequency: 1000
log_save_tb: true

# video recorder
save_video: False

# setups
seed: 11122

# Environment
env: metaworld_reach-v2

gradient_update: 1

# hydra configuration
hydra:
    name: ${env}
    run:
        dir: ./exp/${env}/H${diag_gaussian_actor.params.hidden_dim}_L${diag_gaussian_actor.params.hidden_depth}_lr${agent.params.actor_lr}/teacher_b${teacher_beta}_g${teacher_gamma}_m${teacher_eps_mistake}_s${teacher_eps_skip}_e${teacher_eps_equal}/label_smooth_${label_margin}/schedule_${reward_schedule}/${experiment}_vlm_label_${vlm_label}_generate_check_${generate_check}_double_check_${double_check}_vlm_feedback_${vlm_feedback}_save_equal_${save_equal}_better_traj_gen_${better_traj_gen}_traj_action_${traj_action}_process_type${process_type}_inter${num_interact}_maxfeed${max_feedback}_seg${segment}_Rbatch${reward_batch}_Rupdate${reward_update}_sample${feed_type}_large_batch${large_batch}_seed${seed}